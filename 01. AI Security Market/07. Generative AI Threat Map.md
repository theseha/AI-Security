📅 2025.07.29

# 🌐 생성형 AI 보안 위협 지도 (Generative AI Threat Map)

최근 다양한 업무와 일상에 생성형 AI(Generative AI)를 통합하는 사례가 늘어나고 있는 가운데, 그만큼 **보안 위협 또한 정교하고 복합적으로 진화**하고 있습니다. AI 시스템은 단일한 기술 요소가 아닌, 플랫폼, 애플리케이션, 사용자 상호작용 등 **여러 보안 계층**이 맞물려 있는 구조입니다. 따라서 AI 보안을 위해서는 전체 생명주기를 아우르는 다층적 접근 방식이 필요합니다.

아래는 Microsoft에서 제시한 *Generative AI Threat Map*을 중심으로, 각 위협 요소를 설명합니다.

<img width="1280" height="707" alt="image" src="https://github.com/user-attachments/assets/6a829a8f-533a-444f-8c33-fdf6fa15b5b9" />

---

## 🔐 AI 플랫폼 보안 (AI Platform Security)
### 🧪 1. 학습 데이터 오염 (Training Data Poisoning)
- 공격자가 학습 데이터를 조작하여 모델 성능을 의도적으로 왜곡시키는 방식입니다.
- 가짜 정보 삽입, 기존 데이터 왜곡, 일부 삭제 등을 통해 편향이나 오작동을 유도합니다.
- **징후**: 예기치 않은 성능 저하, 비정상적인 예측, 편향된 결과 등이 나타납니다.

### 🕵️ 2. 모델 탈취 및 역공학 (Model Theft)
- AI 모델의 가중치(weight)나 구조를 유출하여, **지적 재산권 침해**나 **모델의 악용** 가능성이 생깁니다.
- 민감한 모델 내부 정보가 외부로 노출될 경우, 공격자는 해당 모델을 역설계하거나 모방할 수 있습니다.

---

## 🛠️ AI 애플리케이션 보안 (AI Application Security)
### 🗡️ 1. 간접 프롬프트 인젝션 (Indirect Prompt Injection)
- 공격자가 사용자 입력이나 외부 문서에 악의적인 프롬프트를 심어두면, AI가 이를 처리할 때 **원하지 않는 행동을 하게 유도**할 수 있습니다.
- 웹 콘텐츠, 이메일, 협업 문서 등에서 간접적으로 주입되는 경우가 많아 방어가 어렵습니다.

### 💾 2. 데이터 유출/탈취 (Data Exfiltration)
- AI 시스템을 통해 내부 민감 정보를 외부로 유출하는 공격입니다.
- 공격자는 AI를 활용해 민감 데이터를 식별하거나 자동화된 방식으로 외부로 전송할 수 있습니다.

### 🔌 3. 취약한 플러그인 설계 (Insecure Plugin Design)
- LLM(대형 언어 모델) 기반 앱에 통합된 플러그인이 보안 검토 없이 설계된 경우, 악성 요청을 우회적으로 실행할 수 있는 **공격 경로가 열리게 됩니다**.

---

## 🙋 AI 사용 보안 (AI Usage Security)
### 🔓 1. 민감 정보 노출 (Sensitive Information Disclosure)
- 사용자가 의도치 않게 개인정보, 기밀 정보 등을 AI에게 제공하거나, AI가 이를 다른 사용자에게 노출할 수 있습니다.

### 🕶️ 2. 비인가 LLM/플러그인 사용 (Shadow IT / Harmful Plugin)
- 조직 외부에서 승인되지 않은 LLM 앱이나 플러그인을 사용하는 것은 **데이터 유출 및 보안 통제를 우회**할 수 있는 위협이 됩니다.

### 🧨 3. 프롬프트 탈옥 (Jailbreak)
- 공격자가 AI 모델이 설정된 제약(guardrail)을 우회할 수 있도록 **의도된 입력 시퀀스를 구성**하는 방식입니다.
- 특히 최근에는 **Crescendo 공격**처럼 다단계로 우회하는 정교한 방법이 등장하고 있습니다.
  - Crescendo는 여러 단계에 걸쳐 무해한 프롬프트처럼 보이지만 점점 AI를 의도하지 않은 방향으로 유도합니다.

---

## ⚠️ 확장된 AI 보안 위협 (Extended AI Risks)
- **AI 내부자 위험(AI Insider Risk)**: 내부 직원이나 협력자가 AI 시스템을 악용할 가능성.
- **멀티모달 오용**: 텍스트, 이미지, 음성 등 다양한 입력/출력을 조합해 **기존 탐지 체계를 우회**하는 공격.
- **과도한 의존(Overreliance)**: AI 결과를 검증 없이 신뢰해 잘못된 판단이나 의사결정을 유발할 수 있습니다.

---

## 🔎 마무리
생성형 AI의 도입은 큰 기회를 제공하지만, 그만큼 보안 위협도 다면적으로 존재합니다. 위협 요소를 플랫폼, 애플리케이션, 사용자 계층으로 나누어 체계적으로 관리해야 하며, 지속적인 **위협 탐지, 방어 전략 수립, 훈련 데이터 점검**이 필수적입니다.

---

> 참고 자료  
- Microsoft Security Blog  
- CrowdStrike: What Is Data Poisoning?  
- RAND: Securing AI Model Weights  
- OWASP Top 10 for LLM, ML  
- MITRE ATLAS, MSRC AI Bug Bar
