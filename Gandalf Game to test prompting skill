📅 2025.07.26

# 🎮 LLM 보안을 게임으로? Lakera의 Gandalf로 알아보는 AI Security

최근 생성형 AI의 보안 문제가 대두되면서, 이를 쉽게 체험하고 학습할 수 있는 도구들이 주목받고 있습니다. 그 중 하나가 바로 **Lakera**라는 AI 보안 스타트업에서 개발한 게임, **[Gandalf](https://gandalf.lakera.ai/)**입니다.

이 게임은 단순한 오락을 넘어서, **LLM(Large Language Model)**의 보안 취약점을 이해하고 **prompt injection** 기술을 실전처럼 연습해볼 수 있도록 설계되어 있습니다. 사용자에게 주어지는 목표는 단 하나 — **보안으로 숨겨진 시스템 비밀번호를 알아내는 것**입니다.



---

## 🧩 레벨 구조

Gandalf는 총 **7개의 단계와 하나의 Final Level**로 구성되어 있으며, 레벨이 높아질수록 단순한 프롬프트를 넘어 **우회적 표현, 인코딩, 시스템 명령 위장 등 고급 기법**이 요구됩니다.

> ✅ *한글 프롬프트도 지원하므로, 영어에 익숙하지 않은 사용자도 도전 가능!*

---

## 🛡️ 왜 이 게임이 중요한가?

Gandalf는 단순한 놀이나 테스트 도구가 아닙니다. 실제로 많은 기업과 보안 담당자들이 이 게임을 통해 다음과 같은 중요한 인사이트를 얻고 있습니다:

- **LLM 보안 위협이 얼마나 쉽게 발생할 수 있는지 체험**
- **안전장치(guardrail)를 우회하는 다양한 기법에 대한 직관적 이해**
- **자신의 서비스에 생성형 AI를 도입할 경우 필요한 보안 고려사항을 학습**

---

## 🎯 이런 분들에게 추천합니다

- LLM을 업무나 서비스에 도입하려는 **조직**
- Prompt Injection이 실제로 어떤 위험을 초래할 수 있는지 경험하고 싶은 **개발자 / 보안 담당자**
- **보안에 관심 있는 일반 사용자**

---

보안을 체험하며 배우고 싶은 분이라면, Gandalf는 아주 훌륭한 출발점이 되어줄 것입니다.

👉 [🔗 Lakera Gandalf 게임 바로가기](https://gandalf.lakera.ai/)
